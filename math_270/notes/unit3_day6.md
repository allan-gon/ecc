# Summary 5.1, 5.2
- $V$ is a vector space
- Inner product $<u, v> \rightarrow \R$
    - $<u, u> \le 0$ **AND** $<u, u> = 0 \Leftrightarrow u = 0_V$
    - $<u, v> = <v, u>$
    - $<ku, v> = k <u, v>$
    - $<u + v, w> =  <u, w> + <v, w>$
- $\lVert u \rVert = \sqrt{<u, u>}$
- $u \perp v \Leftrightarrow <u, v> = 0$
- `thm`: $v_1 \dots v_n$ orthogonal and nonzero $\Rightarrow$ LI
- `def`: projection of $v$ onto $u$: $P(v, u) = \frac {<v, u>}{<u, u>} u$
- `thm`: $v_1 \dots v_n$ orthogonal basis for $V$ then $v \in V$ is represented as: $v = P(v, v_1) + P(v, v_2) + ... + P(v, v_n)$

## Example
- assume we have $u, v, w$ in some IP space $V$. Assume $\lVert u \rVert = 3, <u, v> = 5, <v, w> = -2, <u, w> = -1$
- Calc $<3u + 2v, 2w - 4u>$
    - $<3u, 2w - 4u> + <2v, 2w - 4u>$
    - $<3u, 2w> + <3u, -4u> + <2v, 2w> + <2v, - 4u>$
    - $6 <u, w> -12 <u, u> + 4 <v, w> - 8 <v, u>$
    - $6(-1) -12(9) + 4(-2) -8(5) = -162$

# Section 5.3 Gram-Schmidt Process
- transforming a random basis into an othogonal basis under whatever IP
- `thm`: let $v, u \in$ IP space $V$. Then the vectors $v - P(v, u) \perp u$

## Proof
- NTS $<v - P(v, u), u> = 0$
- $<v - P(v, u), u> = <v - \frac {<v, u>}{<u, u>} u, u>$
- $<v, u> + <- \frac {<v, u>}{<u, u>} u, u>$
- $<v, u> + - \frac {<v, u>}{<u, u>} <u, u>$
- $<v, u> - <v, u> = 0$
- $QED$

## Gram Schmidt Process
- let $v_1 \dots v_n$ be a basis for IP space $V$
- construct $u_1 \dots u_n$ tp be an orthogonal basis for the IP space $V$
    - $u_1 = v_1$
    - $u_2 = v_2 - P(v_2, u_1)$
    - $u_3 = v_3 - P(v_3, u_2) - P(v_3, u_1)$
    - $\vdots$
    - $u_n = v_n - P(v_n, u_{n-1}) - \dots - P(v_n, u_1)$

### Example
- consider the IP space $\R^3$ with IP $<(a, b, c), (d, e, f)> = 2ad + be + 2cf$
- consider the set of vectors $\{ (1, 1, 1), (2, 1, 0), (0, 3, 1)\}$
- verify basis for $\R^3$
    - len $= \dim$
    - $\left(\begin{array}{ccc|c} 1 & 2 & 0 & 0 \\ 1 & 1 & 3 & 0 \\ 1 & 0 & 1 & 0 \end{array}\right)$
    - ref: $\left(\begin{array}{ccc|c} 1 & 2 & 0 & 0 \\ 0 & -1 & 3 & 0 \\ 0 & 0 & -5 & 0 \end{array}\right)$
    - unique sol so LI so apanning and because len = dim, basis
- construct orthogonal basis for $\R^3$ in this IP space
    - $u_1 = (1, 1, 1)$
    - $u_2 = (2, 1, 0) - P((2, 1, 0), (1, 1, 1))$
        - $P = \frac {<(2, 1, 0), (1, 1, 1)>}{<(1, 1, 1), (1, 1, 1)>} (1, 1, 1)$
        - $\Rightarrow \frac {5}{5} (1, 1, 1)$
        - $u_2 = (1, 0, -1)$
    - $u_3 = (0, 3, 1) - P((0, 3, 1), (1, 0, -1)) - P((0, 3, 1), (1, 1, 1))$
        - repeating we get: $(0, 3, 1) + \frac {1}{2}(1, 0, -1) - (1, 1, 1)$
        - $u_3 = (\frac {-1}{2}, 2, \frac{-1}{2})$

# Section 6.1 Linear Transformation
- `def`: let $V, W$ be vector spaces. Then a mapping $T$ is called a linear transformation written as $T:V \rightarrow W$ if
    - $T(v_1 + v_2) = T(v_1) + T(v_2)$
    - $T(kv) = k T(v)$

## Example
- prove that the mapping $T: P_3 (\R) \rightarrow P_2 (\R)$ defined by $T(p(x)) = p'(x) - p''(x)$
- let $f(x), g(x) \in P_3 (\R)$
- NTS: $T(f(x) + g(x)) = T(f(x)) + T(g(x))$
    - $T(f(x) + g(x)) = f'(x) + g'(x) - f''(x) - g''(x)$
    - $T(f(x)) + T(g(x)) = f'(x) - f''(x) + g'(x) - g''(x)$
    - $QED$
- NTS: $T(kf(x)) = kT(f(x))$
    - $T(kf(x)) = k (f'(x) - f''(x))$
    - $kT(f(x)) = k (f'(x) - f''(x))$
    - $QED$

## Example Disproving
- consider the mapping $T: M_2 (\R) -> \R^2$ defined by $T(A) = (tr(A), det(A))$
- consider $T(\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}) = (4, 4)$
- $T( 2 \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}) = (4, 4) = 2(2, 1) = (4, 2)$
    - rule broke.
    - **NOTE** usually with matrices, determinant breaks

# Section 6.3/6.4
- if you $T(\vec 0_V)$ you are gauranteed to get $T(\vec 0_W)$
- `kernel`: $Ker(T) = \{ v \in V: T(v) = \vec 0_w \}$
    - subspace of $V$
- `range`: $Rng(T) = \{ w \in W: \exists v \in V T(v) = w \}$
    - subspace of $W$
    - really just all of the outputs generated by the transformation
- `General rank nullity thm`: $T: V \rightarrow W$ a linear transformation. Suppose $\dim (V) = n$ then $\dim (Ker(T)) + \dim(Rng(T)) = \dim (V)$
    - $\dim (Rng(T)) = \dim (V) - \dim (Ker(T))$

## Example
- define the LT $T: M_2 (\R) \rightarrow \R^5$ by $T(\begin{bmatrix} a & b \\ c & d \end{bmatrix}) = (a + d, 2a + 2d, b -c, a + b - c + d, c - b)$
- evaluate $T(\begin{bmatrix} 1 & 3 \\ 2 & 0 \end{bmatrix})$
    - $(1, 2, 1, 2, -1)$
- Determine $Ker(T)$, a basis for $Ker(T)$ and $\dim (Ker(T))$
    - what things map to the $\vec 0$
    - let $v \in Ker(T); T(v) = \vec 0_W$
    - $T(\begin{bmatrix} a & b \\ c & d \end{bmatrix}) = (0, 0, 0, 0, 0)$
    - $(a + d, 2a + 2d, b -c, a + b - c + d, c - b) = (0, 0, 0, 0, 0)$
    - $\left(\begin{array}{cccc|c} 1 & 0 & 0 & 1 & 0 \\ 2 & 0 & 0 & 2 & 0 \\ 0 & 1 & -1 & 0 & 0 \\ 1 & 1 & -1 & 1 & 0 \\ 0 & -1 & 1 & 0 & 0 \end{array}\right)$
    - ref: $\left(\begin{array}{cccc|c} 1 & 0 & 0 & 1 & 0 \\ 0 & 1 & -1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \end{array}\right)$
    - inf sol with 2 param: $(-s, t, t, s)$
    - $Ker(T) = \{ \begin{bmatrix} -s & t \\ t & s \end{bmatrix} \}$
    - basis: $\{ \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \}$
    - $\dim (Ker(T)) = 2$
- repeat b for $Rng(T)$
    - $\dim(Rng(T)) = \dim(V) - \dim(Ker(T)) = 4 - 2 = 2$
    - $Rng(T) = \{ (a + d, 2a + 2d, b -c, a + b - c + d, c - b) \}$
        - but can param: $Rnf(T) = \{ (t, 2t, s, t + s, -s) \}$
    - basis: $\{ (1, 2, 0, 1, 0), (0, 0, 1, 1, -1) \}$

# Talk to him about the project after class
- the pca was just for visual and clustering so i could hopefully
- simply label on the clusters but unfortunately there's quite a bit of overlap. that being said this is a much better starting point for seperating images into folders ergo labeling
- should i know how to cofactor expand for the wronskian cause i missd that
    - yes
- is passing a kernel over an image related to this kernel?
    - dunno what that actually means but it usually is for feature extraction like edge detection, gaussian blur, etc